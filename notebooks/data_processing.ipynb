{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add06e1c",
   "metadata": {},
   "source": [
    "## What is this notebook used for ?\n",
    "\n",
    "The purpose of this notebook is to prepare the data to fit the machine learning model. \n",
    "To do this, I will first split the raw data into 2 sets, a training set and a test set. \n",
    "\n",
    "I will then create data pre-processing pipelines that will contain basics steps to prepare the data.\n",
    "\n",
    "\n",
    "## How is it done?\n",
    "\n",
    "I use scikit-learn, especially train_test_split & ColumnTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c55018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49004a2f",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c6a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projets personnels\\employee_leave\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f18368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use os for the paths so that this code works on differents OS.\n",
    "path_data = os.path.join('data', 'raw', 'Employee.csv')\n",
    "df_raw = pd.read_csv(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3daa3ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender',\n",
       "       'EverBenched', 'ExperienceInCurrentDomain', 'LeaveOrNot'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef0fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "col_X = ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender',\n",
    "       'EverBenched', 'ExperienceInCurrentDomain']\n",
    "\n",
    "# output data (target)\n",
    "col_y = ['LeaveOrNot']\n",
    "\n",
    "\n",
    "X = df_raw[col_X]\n",
    "y = df_raw[col_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e682a",
   "metadata": {},
   "source": [
    "## Split the data in training & test sets\n",
    "\n",
    "At this level of work it is very important to create a test set and not work on it by storing it somewhere. \n",
    "\n",
    "In addition, the \"random_state\" parameter must be set so that the result is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0284135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data are dataframes (before & after spliting them)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cce2a5",
   "metadata": {},
   "source": [
    "### Saves the data in human readable format (csv) using pd.dataframes to_csv() method\n",
    "\n",
    "An important note is that we store data in csv format and not in pickle format for example because our data are readable, if we haven't storage size constraints it's important to store human readable data in human readable file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906ea6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_folder = os.path.join('data', 'training')\n",
    "test_folder = os.path.join('data', 'test')\n",
    "\n",
    "# export train data\n",
    "X_train.to_csv(os.path.join(train_folder, 'x_train.csv'))\n",
    "y_train.to_csv(os.path.join(train_folder, 'y_train.csv'))\n",
    "\n",
    "# export test data\n",
    "X_test.to_csv(os.path.join(test_folder, 'x_test.csv'))\n",
    "y_test.to_csv(os.path.join(test_folder, 'y_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b4178",
   "metadata": {},
   "source": [
    "## Data processing pipeline\n",
    "\n",
    "The steps that we will put in this pipeline are :\n",
    "- put the categorical variables in **1-hot coding**\n",
    "- encode the binary variables into numerical binary variables\n",
    "- do a standardization of the numerical variables (in a specific pipeline for models that are sensitive to this).\n",
    "\n",
    "These two first steps are necessary if we want to use a model on these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea8cb0",
   "metadata": {},
   "source": [
    "### One-hot variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca92499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoder\n",
    "one_hot = OneHotEncoder()\n",
    "# categorial variables\n",
    "col_one_hot = ['Education', 'City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91fcacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3117, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.fit_transform(X_train[col_one_hot]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b320a5d",
   "metadata": {},
   "source": [
    "_Note:_ As reminder, the \"Education\" and \"City\" variables both have 3 variables, so this gives 6 columns once the one-hot encoding is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a83186",
   "metadata": {},
   "source": [
    "### Numerical binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07e8814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_binary = OrdinalEncoder()\n",
    "col_binary = ['Gender', 'EverBenched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "883e252b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3117, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_binary.fit_transform(X_train[col_binary]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb43ca",
   "metadata": {},
   "source": [
    "_Note:_  We keep the same number of columns here because we are transforming categorical values into numeric (here we use OrdinalEncoder() to transform into a binary variable because the first 2 values are 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a45df72",
   "metadata": {},
   "source": [
    "### Standardization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded46f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_numerical = [col for col in col_X if col not in col_one_hot + col_binary + ['PaymentTier']]\n",
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a5b7e",
   "metadata": {},
   "source": [
    "_Note:_ \n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "with u = mean of a variable & s = standard devition .\n",
    "\n",
    "## Preprocessing steps\n",
    "\n",
    "\n",
    "Here there will be 2 pipelines created: \n",
    "- the first will only format the categorical data into a numerical representation\n",
    "- the second one will do an additional step of normalization of the numerical data.\n",
    "\n",
    "**Why make these 2 pipelines?**\n",
    "\n",
    "\n",
    "Because some of the models we are going to use later on need a normalisation of the numerical data (logistic regression) and others do not (decision tree models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604c271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = ColumnTransformer([\n",
    "                                            ('one_hot', one_hot, col_one_hot),\n",
    "                                            ('to_binary', ord_binary, col_binary)\n",
    "                                            ])\n",
    "\n",
    "preprocessing_pipeline_linear_model = ColumnTransformer([\n",
    "                                            ('one_hot', one_hot, col_one_hot),\n",
    "                                            ('to_binary', ord_binary, col_binary),\n",
    "                                            ('numerical', std_scaler, col_numerical)\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c402f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline\\\\preprocessing\\\\preprocessing_linear_model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the pipelines\n",
    "prepared_col = preprocessing_pipeline.fit_transform(X_train)\n",
    "prepared_col_lin = preprocessing_pipeline_linear_model.fit_transform(X_train)\n",
    "\n",
    "# save the pipelines\n",
    "output_path = os.path.join('pipeline', 'preprocessing', 'preprocessing_model.pkl')\n",
    "output_path_lin = os.path.join('pipeline', 'preprocessing', 'preprocessing_linear_model.pkl')\n",
    "\n",
    "joblib.dump(preprocessing_pipeline, output_path)\n",
    "joblib.dump(preprocessing_pipeline_linear_model, output_path_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6438badd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one_hot', OneHotEncoder(), ['Education', 'City']),\n",
       " ('to_binary', OrdinalEncoder(), ['Gender', 'EverBenched']),\n",
       " ('remainder', 'drop', [1, 3, 4, 7])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformers can be accessed \n",
    "preprocessing_pipeline.transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb9bc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3117, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: we have 3x2+2 = 8 columns\n",
    "prepared_col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cab1047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape:  (3117, 8)\n",
      "Data shape after drop:  (3117, 4)\n"
     ]
    }
   ],
   "source": [
    "# raw data shape\n",
    "print(\"Raw data shape: \", X_train.shape)\n",
    "\n",
    "# Remove prepared columns from the raw data\n",
    "X_train_drop = X_train.drop(columns=col_binary + col_one_hot)\n",
    "\n",
    "# raw data shape\n",
    "print(\"Data shape after drop: \", X_train_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "362ece68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prepared columns to the raw data\n",
    "X_prepared = np.concatenate((X_train_drop, prepared_col), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "390e97de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3117, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151c98e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we have formatted the data that was not directly exploitable by a Machine Learning model.\n",
    "\n",
    "These elementary steps are stored in 1 function in the script src.preprocessing.pipeline_preprocessing.py \n",
    "\n",
    "We can now move on to the modelling of the interactions between the variables and the Machine Learning (prediction) part!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
